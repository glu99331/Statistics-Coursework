---
title: "STAT 1361 - Homework 4"
author: "Gordon Lu"
date: "2/18/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 2:

ISLR Conceptual Exercise 2

### a)
If $X$ is uniformly distributed, then (0.65-0.55)/(1-0) = 10%.

### b) 
For two features, $10\% \times 10\%  = 1\%$.

### c) 
On average, $0.10^{100} \times 100 = 10^{-98} \%$

### d) 

When there are a large number of dimensions, the percentage of observations that can be used to predict with KNN becomes very small. This means that for a set sample size, more features leads to fewer neighbors.

### e) 
$$
p = 1, l = 0.10
$$
$$
p = 2, l = \sqrt(0.10) \times 0.32
$$
$$
p = 3, l = 0.10^{1/3} \times 0.46
$$
$$
...
$$
$$
p = N, l = 0.10^{1/N}
$$
When p = 1, length = 0.1

When p = 2, length = $0.1^{1/2} = 0.316$

When p = 100, length = $0.1^{1/100} = 0.977$.

When the number of features is high (i.e. p=100), to use on average 10% of the training observations would mean that we would need to include almost the entire range of each individual feature.

### 2b)
In exercise 4, we were observing KNN. To argue against the statemnt, "non-parametric approaches often perform poorly when p is large," we can make the claim that if a hypercube's dimensions increase as p increases, the classfier should be able to predict samples efficient, regardless of the size of p. Generally for non-parametric appraoches, if there are a lot of features, but few samples, it will be difficult to extrapolate predictions based on nearby samples. However, if there are a lot of samples, a test observation should be able to extrapolate from nearby previously observed samples, which is not the case with few samples. Thus, the generalization is not necessarily always true.

### 2c)
In high-dimensional space, you are often forced to **overfit**.

### 2d) 
There are two valid answers to this question. Generally, more data is not a bad thing. This follows from the idea of distributions and the Central Limit Theorem; the more data you have, the better you can observe the underlying distribution of the data. As such, you can create a model that fits the data even better. On the other hand, the more data that enters the model, the higher the probability is to introduce random noise into the dataset. In turn, this can mess up models, damage inferential procedures and accuracies. Generally, sticking to the first approach is a good rule of thumb, but it always has the potential to negatively impact the modeling.

## Exercise 3:

ISLR Conceptual Exercise 5

### 5a)
With a linear Bayes decision boundary, we expect QDA to be better than LDA for the training set. QDA is generally more flexible than LDA, so it will likely have a closer/better fit than LDA. On the test set, LDA is expected to be better than QDA, since there is the risk of overfitting the linearity on the decision boundary.

### 5b)
Since the study has a non-linear decision boundary, QDA is expected to be better than LDA on both the training and testing sets, since there is a lower risk of overfitting to data with the new assumption.

### 5c)
As the sample size increases, we expect the test prediction accuracy of QDA to improve relative to LDA, since the variance of data is less of a concern, so overfitting in turn is also less of a concern. QDA is generally more flexible and has more variance than LDA.

### 5d)
False. When the sample size is small, QDA may lead to overfitting, leading to a lower test accuracy.

ISLR Conceptual Exercise 8

8) In KNN, with K = 1, the training error rate is 0%. Since the average of training error and testing error is 18%, (0% + x%)/2 = 18%, where `x` is the testing error percentage. We can find the test error to be 36%, which is higher than the logistic regression testing error. As such, we can explain this KNN situation as overfitting. Thus, we want to use logistic regression.

## Exercise 4:

ISLR Applied Exercise 11

### 11a) 

```{r problem11a, results = 'hide'}
auto <- read.csv(file = 'C:/Users/gordo/Desktop/Auto.csv')
mpg01 <- rep(0, length(auto$mpg))
mpg01[auto$mpg > median(auto$mpg)] <- 1
auto <- data.frame(auto, mpg01)
auto
```

```{r problem11b, fig.align='center'}
cor(auto[,-9])
pairs(auto)
```
`displacement`, `horsepower`, `weight`, and `acceleration` seem to be highly correlated with `mpg01`. 

```{r problem11c}
train = (auto$year%%2 == 0)
auto.train = auto[train, ]
auto.test = auto[!train, ]
mpg01.test = mpg01[!train]
```

```{r problem11d}
library(MASS)
fit.lda <- lda(mpg01 ~ cylinders + weight + displacement
               + horsepower, data = auto, subset = train)
fit.lda
pred.lda <- predict(fit.lda, auto.test)
table(pred.lda$class, mpg01.test)
mean(pred.lda$class != mpg01.test)
```
The test error is: 12.6374%.

### 11e)
```{r problem11e}
fit.qda <- qda(mpg01 ~ cylinders + weight + displacement
               + horsepower, data = auto, subset = train)
fit.qda
pred.qda <- predict(fit.qda, auto.test)
table(pred.qda$class, mpg01.test)
mean(pred.qda$class != mpg01.test)
```
The test error is: 13.1868%.

### 11f)
```{r problem11f}
fit.glm <- glm(mpg01 ~ cylinders + weight + displacement
               + horsepower, data = auto, family = binomial, subset = train)
fit.glm
probs <- predict(fit.glm, auto.test, type = "response")
pred.glm <- rep(0, length(probs)) 
pred.glm[probs > 0.5] <- 1
table(pred.glm, mpg01.test)
mean(pred.glm != mpg01.test)
```
The test error is: 12.0879%.

### 11g)
```{r problem11g}
library(class)
training <- cbind(auto$cylinders, auto$weight, auto$displacement, auto$horsepower)[train,]
testing <- cbind(auto$cylinders, auto$weight, auto$displacement, auto$horsepower)[!train,]
train.mpg01 <- mpg01[train]
set.seed(1)
pred.knn <- knn(training, testing, train.mpg01, k = 1)
table(pred.knn, mpg01.test)
mean(pred.knn != mpg01.test)
```

```{r problem11g2}
pred.knn <- knn(training, testing, train.mpg01, k = 5)
table(pred.knn, mpg01.test)
mean(pred.knn != mpg01.test)
```

```{r problem11g3}
pred.knn <- knn(training, testing, train.mpg01, k = 25)
table(pred.knn, mpg01.test)
mean(pred.knn != mpg01.test)
```
When K = 1, the test error is 15.3846%.

When K = 5, the test error is 14.8352%.

When K = 25, the test error is 14.2857%.

Thus, K = 25 performs the best among K = 1, 5, and 25 on this dataset.

## Exercise 5)

### 5a)
```{r problem5a, fig.align = 'center'}
example(UCBAdmissions)
```

The first plot implies that males make up a significantly higher proportion of the admitted students compares to females. Meanwhile, males and females make up about the same proportion of rejected (50/50 split). Therefore, yes, there seems to be a bias towards males being admitted more than females. The overall percentage of men that were accepted is 44.5188%, while the percentage for woman is 30.3542%.

### 5b) 
The 6 plots show that none of the departments are inherently biased. By comparing the admitted box splits to the rejected box splits, this is evident. If there were a serious imbalance when comparing admitted to rejected, then there would be bias similar to (a). However, the six plots do not show such a trend.

### 5c)
The general idea of Simpson's Paradox is that a specific trend appears when there is a lot of data aggregated, but then completely disappears when the data is split into their respective subgroups.

### 5d) 
This can be explained by claiming females applied to extremely competitive departments that inherently have low acceptance rates, while men typically applied to less competitive departments with higher acceptance rates. Thus, more men ended up getting into their desired departments anways, increasing the number of overall mean accepted compared to females.

### 5e)
```{r problem5e}
data(UCBAdmissions)
Adm <- as.integer(UCBAdmissions)[(1:(6*2))*2-1]
Rej <- as.integer(UCBAdmissions)[(1:(6*2))*2]
Dept <- gl(6,2,6*2,labels=c("A","B","C","D","E","F"))
Sex <- gl(2,1,6*2,labels=c("Male","Female"))
Ratio <- Adm/(Rej+Adm)
berk <- data.frame(Adm,Rej,Sex,Dept,Ratio)
LogReg.gender <- glm(cbind(Adm,Rej)~Sex,data=berk,family=binomial("logit"))
summary(LogReg.gender)
```
We can say the admission rate of females is statistically significant with p-vale < 2e-16. Thus, there is in fact a bias against females.

### 2f)
```{r, exercise5f}
LogReg.gender <- glm(cbind(Adm,Rej)~Sex+Dept,data=berk,family=binomial("logit"))
summary(LogReg.gender)

```
Once departments are taken into consideration, we can see that the female acceptance rate is no longer statistically significant, so there is no bias against females. The coefficient for females also dropped from -0.6104 to 0.0999, which is closer to 0 compared to (e), making it less of a factor.

Overall, we showed Simpson's Paradox, which is the idea that aggregated data showing specific trend, but subgroups of the data not showing such trends. Thus, though the data may seem biased, it may actually be balanced, and further inference of the data can lead to a high-level reaosning about this paradox. One can discover the root of the biases by either plotting the subgroups as in (a) and (b), or by fitting a model to the data, and performing hypothesis testing on the predictors to determine whether they are statistically significant.


