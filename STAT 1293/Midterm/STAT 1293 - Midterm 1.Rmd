---
title: "STAT 1293 - Midterm I"
author: "Gordon Lu"
date: "7/13/2020"
output:
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1: Objects (25 points)

### 1a) Create a vector as follows. Call it `v`. (2 points)

### Solution:
```{r, eval = TRUE}
v <- c(0, 1, -2, 2, -1, 0)
v
```

### 1b) Create a matrix as follows, call it `M`. Don't forget the column names. (5 points)

### Solution:
```{r, eval = TRUE}
c1 <- c("1", "2", "3", "4", "5")
c2 <- letters[21:25]
c3 <- c("TRUE", "TRUE", "FALSE", "FALSE", "FALSE")
M <- cbind(c1,c2,c3)
M
```

### 1c) Create a data frame as follows. Call it `df`. (4 points)

### Solution:
```{r, eval = TRUE}
df <- data.frame(c1,c2,c3)
df
```

### 1d) Create a list as follows. Call it `mylist`. (5 points)

### Solution:
```{r, eval = TRUE}
mylist <- list(v, M[, c(1,3)], df)
mylist
```

### 1e) Pull the third row of `df`. (2 points)

### Solution:
```{r, eval = TRUE}
df[3, ]
```

### 1f) Pull out the second sub-list in `mylist` (2 points)

### Solution:
```{r, eval = TRUE}
mylist[[2]]
```

### 1g) Convert `v` to a factor-type vector (call it `v.f`) and redefine the levels as “Strongly Disagree”,“Disagree”, “Neutral”,“Agree”,“Strongly Agree”, with -2 being “Strongly Disagree” and 2 being “Strongly Agree”. (5 points)

### Solution:
```{r, eval = TRUE}
v.f <- factor(v, levels = -2:2)
levels(v.f) <- c("Strongly Disagree","Disagree", "Neutral","Agree","Strongly Agree")
v.f
```

## Problem 2: Probability Distributions (40 points)

### 2a) Find `P(-3 < X < 3)` where `X ~ N(2,2)` (3 points)

### Solution:
```{r, eval = TRUE}
pnorm(3, mean = 2, sd = 2) - pnorm(-3, mean = 2, sd = 2)
```

### 2b) Find `P(X = 15)` where `X ~ Binomial(20, 0.8)` (2 points)

### Solution:
```{r, eval = TRUE}
dbinom(x = 15, size = 20, prob = 0.8)
```

### 2c) Generate 100 observations from the $t_{20}$ and calculate the mean and standard deviation of them. (5 points)

### Solution:
```{r, eval = TRUE}
t_20 <- rt(100, 20)
mean(t_20)
sd(t_20)
```

### 2d) Find the 99th percentile of the $\chi_{5}^{2}$ distribution. (2 points)

### Solution:
```{r, eval = TRUE}
qchisq(0.99, 5)
```

### 2e) Plot the density functions of $F_{20,20}$ and $F_{5,5}$ distributions. (10 points)

### Solution:
```{r, eval = TRUE}
x <- seq(0, 3, 0.01)
d20.20 <- df(x, 20, 20)
d5.5 <- df(x, 5, 5)
plot(x, d20.20, type = "l", lwd = 2, col = 2, xlab = "x", ylab = "PDF", ylim = c(0,1), xlim = c(0, 10))
lines(x, d5.5,lwd = 2, col = 4)
```

### 2f) Generate 1000 observations from the standard normal distribution and the $t_{50}$ distribution, respectively. Create a histogram for each of the two samples. Is there any obvious difference between the two histograms. (10 points)

### Solution:
```{r, eval = TRUE}
n1000 <- rnorm(1000)
t50 <- rt(1000, 50)

par(mfrow = c(1,2), pty = "s")
hist(n1000, xlim = c(-4, 4), ylim = c(0, 0.5), freq = F, main = "Standard Normal Histogram")
hist(t50, xlim = c(-4, 4), ylim = c(0, 0.5), freq = F, main = "t_50 Histogram")
```

No, there is no obvious difference between the two histograms. 

### 2g) Find the 90% quantile of the $t_{10}$ distribution. (2 points) 

### Solution:
```{r, eval = TRUE}
qt(0.90, 10)
```

### 2h) Find $P(t_{15} > 3)$ (3 points)

### Solution:
```{r, eval = TRUE}
1 - pt(3, 15)
```

### 2i) Find $P(X > 15)$ where `X ~ Binomial(20, 0.8)` (3 points)

### Solution:
Since `pbinom()` calculates $P(X \leq 15)$, and $P(X > 15)$ is analogous to asking: $1 - P(X \leq 15)$, we can do `1 - pbinom(15, 20, 0.8)`.
```{r}
1 - pbinom(15, 20, 0.8)
```

## Problem 3: Sampling Distributions (25 points)

Part a:

### 3ai) Generate 1000 samples of each of three sample sizes: n = 1, n = 10, and n = 30. Calculate the sample means of the 1000 samples of each sample size. (3 points)

### Solution:
```{r, eval = TRUE}
m <- 1000 #number of samples

#sample size 1
# x_bar1 <- rnorm(m, 100, 15) #N(100, 15)
# 
# n <- 10 #Sample size 10
# x_bar10 <- rep(0, m)
# for(i in 1:m){x_bar10[i] <- mean(rnorm(n, 100, 15))}
# 
# n <- 30 #Sample size 30
# x_bar30 <- rep(0, m)
# for(i in 1:m){x_bar30[i] <- mean(rnorm(n, 100, 15))}

#Now store all x_bar into a single vector:
x_bar <- matrix(0, 3, m)

n <- c(1, 10, 30) #Sample sizes are 1, 10, 30

for(i in 1:3) #Fill each of n = 1, n = 10, and n = 30 up
{
  for(j in 1:m) #Fill each with 1000 samples
  {
    #Calculate sample mean for 1000 samples of size 1, 10, 30
    x_bar[i, j] <- mean(rnorm(n[i], 100, 15))
  }
}
```


### 3aii) Calculate the mean of sample means and standard deviation of sample means for each sample size. Do you think the two basic properties of $\overline{X}$ are true? Why? Show your evidence. (6 points)

### Solution:
```{r, eval = TRUE}
apply(x_bar, 1, mean) #calculate mean
rep(100, 4) #compare to theoretical mean
apply(x_bar, 1, sd) #calculate standard error 
15/sqrt(n) #compare to theoretical standard error
```

Yes, the two basic properties of $\overline{X}$ do hold. Notice how the simulated mean and standard error are fairly close to the theoretical mean and standard error. In particular, observe how the sample mean of means, gets closer to the theoretical mean, 100, as the sample size increases, in other words, the differences between the theoretical mean and simulated mean become smaller as the sample size increases. Additionally, observe that in regard to the standard error of the mean, the simulated standard error, when compared vertically to the theoretical standard error, are very close to each other. Thus, the two basic properties of $\overline{X}$ do appear to hold.

### 3aiii) Create a histogram of the sample mneans for each sample size. (4 points)

### Solution:
```{r, eval = TRUE}
par(mfrow = c(1,3), pty = "s")
for(i in 1:3) 
{
  #Plot each of the respective histograms
  hist(x_bar[i,],ylab="Relative Frequency",freq=F,
       main=paste("n=",n[i]),xlab="sample mean", xlim = c(50, 150), ylim = c(0, 0.15)) 
}

```

Part b: This part is to verify the Central Limit Theorem.

### 3bi/3bii/3biii) Simulate 1000 random samples from the $\chi_{3}^{2}$ distribution with 3 sample sizes (5, 30, 100) (2 points), Calculate sample means (2 points), and Create histograms of the sample means (8 points).

### Solution:
```{r, eval = TRUE}
par(mfrow = c(1,3), pty = "s")
m <- 1000
x_bar <- matrix(0, 3, m)
n <- c(5, 30, 100)

for(i in 1:3)
{
  for(j in 1:m)
  {
    x_bar[i,j] = mean(rchisq(n, 3))
  }
  hist(x_bar[i,], freq = F, ylab = "Relative Frequency", main = paste("n=", n[i]), 
       xlab = "sample mean", xlim = c(0, 12), ylim = c(0, 0.40))
}
```

## Problem 4: Numerical Summary and Graphical Display of One Numerical Variable (25 points)

### 4a) Download the data set (call it `iq`) and show the top 5 rows of it. (2 points)

### Solution:
```{r, eval = TRUE}
iq <- read.table("C:/Users/gordo/Desktop/iq.txt", header = TRUE) #read in iq.txt
head(iq, 5)
```

### 4b) Calculate the mean, standard deviation, and variance of IQ. (3 points)

### Solution:
```{r, eval = TRUE}
mean(iq$IQ)
sd(iq$IQ)
var(iq$IQ)
```

### 4c) Calculate the five-number summary of IQ. (2 points)

### Solution:
```{r, eval = TRUE}
summary(iq$IQ)
```

### 4d) Create a histogram of `IQ`. Superimpose a normal density curve on it. (10 points)

### Solution:
```{r, eval = TRUE}
hist(iq$IQ, col = "lightblue", main = "IQ Scores of 7th-Grade Students",
     sub = "Data from 78 7th-grade students in a rural midwestern school",
     freq = F, xlim = c(50, 160), ylim = c(0, 0.045))
y = seq(50, 160)
lines(y, dnorm(y, mean(iq$IQ), sd(iq$IQ)), col = 3, lwd = 2) #overlay density curve
```

### 4e) Create a boxplot of `IQ`. (5 points)

### Solution:
```{r, eval = TRUE}
boxplot(iq$IQ, horizontal = TRUE, col = "orange", outline = TRUE)
```

### 4f) Create a stem-and-leaf plot of IQ and specify the four outliers in the boxplot. (3 points)

### Solution:
```{r, eval = TRUE}
stem(iq$IQ)
```

The outliers of the data are 72, 74, 77, and 79. In particular any value that is less than 81.25 is consider a lower outlier, and anything above 139.25 is also considered an outlier.

## Problem 5: Numerical Summary and Graphical Display of Grouped Numerical Data (25 points)

### 5a) Download the data set `mitt2` and randomly sample 10 rows to view. (3 points)

### Solution:
```{r, eval = TRUE}
mitt2 <- read.table("C:/Users/gordo/Desktop/mitt2.txt", header = TRUE) #read in mitt2.txt
attach(mitt2)
mitt2[sample(nrow(mitt2), 10), ]
```

### 5b) Convert the `attitude` variable to a factor-type vector and use labels "positive" and "negative". Use the `table()` function to summarize the factor variable. (6 points)

### Solution:
```{r, eval = TRUE}
detach(mitt2)
mitt2 <- transform(mitt2, attitude = factor(attitude, labels = c("positive", "negative")))
attach(mitt2)
table(mitt2$attitude) #summarize attitude variable
```

### 5c) Compare the five-number summaries and the means and standard deviations of the two groups. (10 points)

### Solution:
```{r, eval = TRUE}
by(mitt2, mitt2$attitude, summary)
mitt2.positive <- mitt2$trustworthiness[mitt2$attitude == "positive"]
mitt2.negative <- mitt2$trustworthiness[mitt2$attitude == "negative"]
#compute respective sd's.
sd(mitt2.positive)  
sd(mitt2.negative)
```

Which group has higher ratings in general? Which group has a larger range and interquartile range ($Q_3 - Q_1$)? How about standard deviation?
It appears that the `Negative` group tends to have higher ratings. In regards to range, it appears that the `Positive` group has a larger range with `3.7`, while the `Negative` group has a lower range with `3.1`. In regards to standard deviation, the `Positive` group appears to have a larger standard deviation than the `Negative` group. 

Mean and Median: The `Negative` group has a higher mean with `3.917`, while `Positive` group has lower mean with `3.61`. In regard to median, the `Negative` group has a higher median with `3.900`, while `Positive` group has a lower median with `3.80`.

Range: The `Positive` group has a larger range with `3.7`, while the `Negative` group has a lower range with `3.1`.

Interquartile range: The `Negative` group has a higher IQR with `1.2`, while the `Positive` group has a lower IQR with `1`.

Standard devaiation: The `Positive` group has a higher standard deviation with `0.912745`, while the `Negative` group has a lower standard deviation with `0.7960029`.

### 5d) Create a side-by-side boxplot. (6 points) 

### Solution:
```{r, eval = TRUE}
boxplot(mitt2.positive, mitt2.negative, horizontal = TRUE, outline = TRUE, col = c(2,4))
title(main = "Effect of Attitude on Rating of Trustworthiness", 
      sub = "2012 Presidential Election: Romney vs. Obama")
detach(mitt2)

```

## Problem 6: Numerical Summary and Graphical Display of One Categorical Variable (25 points)

### 6a) Create a bar graph for the age distribution of Facebook users. Do the same for Twitter and LinkedIn. (11 points)

### Solution:
```{r, eval = TRUE}
socialnt <- read.table("C:/Users/gordo/Desktop/socialnt.txt", header = TRUE) #read in socialnt.txt

par(mfrow = c(1,3), pty = "s")

facebook <- barplot(socialnt$FacebookPct, col = rainbow(7), ylim = c(0,25),
                    main = "Facebook Age Demographics") #set percent axis lim using ylim
axis(side = 1, labels = socialnt$Age, at = facebook, las = 2)

twitter <- barplot(socialnt$TwitterPct, col = rainbow(7), ylim = c(0,25),
                    main = "Twitter Age Demographics") #set percent axis lim using ylim
axis(side = 1, labels = socialnt$Age, at = twitter, las = 2)

linkedin <- barplot(socialnt$LinkedInPct, col = rainbow(7), ylim = c(0,25),
                    main = "LinkedIn Age Demographics") #set percent axis lim using ylim
axis(side = 1, labels = socialnt$Age, at = linkedin, las = 2)
```

### 6b) Compare the three barplots created in part (a). Which social networking site has relatively the youngest users in general? Which age the oldest? (4 points)

### Solution:
Based on the above plots, it appears that `Twitter` has relatively more young users. If we consider the first 3 categories (13 to 17), to (25 to 34), there are fairly more young `Twitter` users than young `Facebook` users. Additionally, in regard, it appears that `LinkedIn` has relatively the oldest users in general. If we consider the last 3 categories (45 to 54) to (over 65), LinkedIn has more older users than `Facebook` and `Twitter` do. Alternatively, we can reach the same conclusion by comparing the medians of each respective bar plot. For `LinkedIn`, it is apparent that the median is higher than that of Twitter and `Facebook's` median. As for `Twitter` and `Facebook`, the medians are approximately equal, however, by looking at the lower halves of the bar plots, it is evident that Twitter has more users captured than `Facebook` in the lower half, thus `Twitter` has relatively the youngest users in general.

### 6c) Create a pie chart for the age distribution of Twitter and LinkedIn. (10 points)

### Solution:
```{r, eval = TRUE}
par(mfrow = c(1,2))
pie(socialnt$TwitterPct, socialnt$Age, col = heat.colors(length(socialnt$Age)),
    main = "Twitter Age Demographics", clockwise = F)
pie(socialnt$LinkedInPct, socialnt$Age, col = heat.colors(length(socialnt$Age)),
    main = "LinkedIn Age Demographics", clockwise = F)
```

Is it easy to compare the age distributions of the three social networking sites? Why? (2 points)
Yes, by using a bar graph, it is far easier to tell that `LinkedIn` has more older users, based on the amount of the data that is captured on the right half of the bar plot. In contrast, it is easy to tell that `Twitter` and `Facebook` have more younger users, by sheer comparison of the left halves of the bar plots to the left half of the bar plot of `LinkedIn` users.

However, it is difficult to tell between `Facebook` and `LinkedIn` which has the youngest users in general, by closely looking at the left-halves of the bar plots, and noting that `Twitter` tends to have more of a percentage captured than `Facebook` users. Additionally, note that by using a pie chart, it is easier to discern between `Twitter` and `Facebook` which has the youngest users. 

## Problem 7: Numerical Summary and Graphical Display of Two Categorical Variables (25 points)

### 7a) Create a two-way table based on the table. You may use the `matrix` function or `cbind` or `rbind` function. Make sure you add appropriate row names and column names. (6 points)

### Solution:
```{r, eval = TRUE}
gaming <- read.table("C:/Users/gordo/Desktop/gaming.txt", header = TRUE) #read in gaming.txt
M <- matrix(gaming$Count, nrow = 2, ncol = 3)
colnames(M) <- c("A&B", "C", "D&F")
rownames(M) <- c("Yes", "No")
```

### 7b) Calculate the marginal tables. (6 points)

### Solution:
```{r, eval = TRUE}
total.row <- margin.table(M, 1)
total.row #Calculate row totals

total.col <- margin.table(M, 2)
total.col #Calculate column totals
```

### 7c) Create a conditional proportion table to examine the effect of playing games on grades. (5 points)

### Solution:
```{r, eval = TRUE}
#Row-wise, so condition is played games, and response is grade
prop.table(M, 1) #Given played games = ??, what is grade?
```

Which is higher? `P("A&B" given "Played games")` or `P("A&B" given "Never played games")`?

It appears that `P("A&B" given "Played games")` is higher with `0.5337201`, while `P("A&B" given "Never played games")` is lower with `0.4778555`.

### 7d) Create a bargaph to display the relationship between playing games and grades. (8 points)

### Solution:
```{r, eval = TRUE}
barplot(M, beside = TRUE, legend.text = TRUE,
        args.legend= list(title = "Played Games"), 
        col = topo.colors(2),
        ylim = c(0, 800), xlab = "Grades")
```

## Problem 8: Inference about a single mean (24 points)

### 8a) Create a Q-Q plot for variable `"Earnings"` and visually check if the normality assumption is satisfied. Add a red reference (diagonal) line. (4 points)

### Solution:
```{r, eval = TRUE}
uber <- read.table("C:/Users/gordo/Desktop/uber.txt", header = TRUE) #read in uber.txt
attach(uber)
qqnorm(Earnings)
qqline(Earnings, col = 2)

```

Yes, it appears that the normality assumption is satisfied.

### 8b) Use Shapiro-Wilk test to check normality. (4 points)

### Solution:
```{r, eval = TRUE}
shapiro.test(Earnings)
```
Yes, based on the Shapiro-Wilk normality test, it appears that the normality assumption is satisfied.

### 8c) Report a 95% confidence interval for the average earnings per hour of New York City Uber drivers. (Use `t.test`) (4 points)

### Solution:
```{r, eval = TRUE}
t.test(Earnings, conf.level = 0.95)$conf.int
```

### 8d) Does the data provide sufficient evidence that the average hourly pay is higher than $30? Use `t.test to do the testing. Based on the test result, write down the 4 steps of the hypothesis test. (12 points)

### Solution:
```{r, eval = TRUE}
t.test(Earnings, alternative = "greater", mu = 30)
detach(uber)
```

4-Step H.T.

Hypothesis: $H_{0}: \mu = 30$ vs. $H_{a}: \mu > 30$ 

Test statistic: $t_{0} = \frac{\overline{x} - \mu_{0}}{s/\sqrt{n}}$ = `4.8625`

P-value: $P(T > t_0) = 1 - P(T < t_0)$ = `2.415e-05`

Conclusion: Reject $H_{0}$, since p-value < 0.05.

## Problem 9: Matched-Pairs Design (12 points)

### 9a) Check normality of the variable `Diff` using a Q-Q plot and the Shapiro-Wilk test. (4 points)

### Solution:
```{r, eval = TRUE}
equiv <- read.table("C:/Users/gordo/Desktop/equiv.txt", header = TRUE) #read in equiv.txt
attach(equiv)
summary(equiv)

qqnorm(Diff)
qqline(Diff, col = 2)
shapiro.test(Diff)
```

### 9b) Conduct a one-sample t test about the difference, `Diff`. What conclusion can you make? (4 points)

### Solution:
```{r, eval = TRUE}
t.test(Diff, conf.level = 0.95)
```

Conclusion: Fail to reject $H_{0}$, since p-value > 0.05.

### 9c) Conduct a matched-pairs t test. What conclusion can you make? (4 points)

### Solution:
```{r, eval = TRUE}
t.test(Paper, Computer, paired = TRUE, conf.level = 0.95)
detach(equiv)
```

Conclusion: Fail to reject $H_{0}$, since p-value > 0.05. The same result as part (b).

## Problem 10: Inference about two means (24 points)

### 10a) Create a horizontal side-by-side boxplot for the two samples. Do you think normality assumption is roughly satisfied? (4 points)

### Solution:
```{r, eval = TRUE}
sad <- read.table("C:/Users/gordo/Desktop/sad.txt", header = TRUE) #read in sad.txt
attach(sad)
summary(sad)
boxplot(Price ~ Group, horizontal = TRUE, outline = TRUE)
```

No, I do not think that the normality assumption is satisfied. Note that the Boxplot for Group `S` looks fairly symmetric, however for Group `N`, the data looks skewed. Based on the Boxplots, it would imply that the data is right-skewed.

### 10b) State the appropriate null and alternative hypotheses for comparing the two groups. (2 points)

### Solution:
Hypothesis: $H_{0}: (\mu_S - \mu_N) = 0$ vs. $H_{a}: (\mu_S - \mu_N) \neq 0$ 

### 10c) Perform the significance test at the $\alpha = 0.05$ level, make sure to report the test statistic and the p-value. What is your conclusion? (8 points)

### Solution:
```{r, eval = TRUE}
#on inspection of boxplots, variances are not equal:
Price.N <- Price[Group == "N"]
Price.S <- Price[Group == "S"]
t.test(Price.N, Price.S, conf.level = 0.95, var.equal = FALSE)
```

Conclusion: Reject $H_{0}$, since p-value < 0.05.

### 10d) Use an F test to check equal-variance assumption at $\alpha = 0.05$ level. (4 points)

### Solution:
```{r, eval = TRUE}
var.test(Price.N, Price.S, conf.level = 0.95)
```

### 10d) Perform the pooled t-test at the $\alpha = 0.05$ level. Compare the test result with that of Part c. Any difference? (6 points)

### Solution:
```{r, eval = TRUE}
t.test(Price.N, Price.S, conf.level = 0.95, var.equal = TRUE)
```

Conclusion: Reject $H_{0}$, since p-value < 0.05. Note that the p-value, t-statistic, and degrees of freedom are different, however we reach the same conclusion as Part c.
